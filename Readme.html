<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC
    "-//W3C//DTD XHTML 1.1 plus MathML 2.0 plus SVG 1.1//EN"
    "http://www.w3.org/2002/04/xhtml-math-svg/xhtml-math-svg.dtd">
<html xml:lang='en' xmlns:svg='http://www.w3.org/2000/svg' xmlns='http://www.w3.org/1999/xhtml'>
<head><meta content='application/xhtml+xml;charset=utf-8' http-equiv='Content-type' /><title>Query Analyzer</title></head>
<body>
<h1 id='query_analyzer'>Query Analyzer</h1>

<ul>
<li>2011s Web Search Engine Hw3</li>

<li>Arthor : Te-Chun Chao</li>

<li>PolyId : 0416545</li>
</ul>

<h2 id='version'>Version</h2>

<p>Running under python 2.7 and OSX 10.6</p>

<h2 id='usage'>Usage</h2>

<ol>
<li>Processing index builder</li>
</ol>

<p>under project directory:</p>

<p>$ python builder.py <span>filename</span></p>

<p>accepted filename is nz2.tar and nz10.tar</p>

<ol>
<li>Initialize the query analyzer</li>
</ol>

<p>after the index file: rindex is built you can start the analyzer by command:</p>

<p>$ python analyzer.py</p>

<p>this will prompt the message:</p>

<p>please input search query:</p>

<p>after enter the query and press enter, the program will return top 10 return result for the query. execution time, and the BM relative score of each result</p>

<h2 id='project_struture'>Project Struture:</h2>

<ul>
<li>cparser.py:</li>
</ul>

<p>wrapper for c parser module. parsing html page into word counts by function parser.parse(url,html)</p>

<ul>
<li>builder.py:</li>
</ul>

<p>extract the tarfile into ./tmp and start parsing all data by calling cparser.parse(url,html) create urltable file for url id reference reduce the data into reverted-index as format :</p>

<p>( <span>word</span> <span>urlid</span> )</p>

<p>sort the reverted-index data and save in tmp/index_<span>id</span> file for future process</p>

<ul>
<li>urltable.py:</li>
</ul>

<p>load the url and id table into memory and manipulate, after data changed, write back into files. provide &#8220;index&#8221; mode for query, &#8220;url&#8221; mode for builder and parser. which laod the table map using urlid as key or using url as key.</p>

<ul>
<li>pipeline.py:</li>
</ul>

<p>read the data from index file and write index-item into file using python struct module, pack and unpack function for binary reading/writing also can set debug = True to enable ANSI read/write</p>

<ul>
<li>merger.py:</li>
</ul>

<p>merge the index files in ./tmp by merge sort, using qheap module for buffer heap, when heap excess 1000000 records or file ended, flush buffer into file and saved file into merge queue for next merge output rindex in project directory as final reverted-index file also output iindex, records the memory position of each alphabat in rindex for the query analyzer to perform faster search.</p>

<ul>
<li>analyzer.py</li>
</ul>

<p>read the query and parse into words, than using binary search to locate the words, calculate the BM score of the word matching, repeat the calculation for each word in query, return the top 10 url to user.</p>

<p>+builder_spec.py</p>

<p>Test cases for builder, make sure the urltable and parsing function can work correctly, can execute by</p>

<p>$ python builder_spec.py</p>

<p>or run in the unittest module</p>

<p>$ python -m unittest builder_spec.BuilderSpec</p>

<h2 id='what_have_been_done'>What have been done</h2>

<p>In homework3, the target is to build a query analyzer with high efficientcy and scalability, also calculate the relation of query and rank the results.</p>

<p>The analyzer basically will parse the query into word query, than read match data from index file save each word and url match in hash, calculate the BM score and add to hash, after calculat all query, return top 10 result to user.</p>

<ul>
<li>index structure</li>
</ul>

<p>The index structure is simple, each word is pairing with the frequency and the docid, and using python struct.pack function to write as binary, make the index-size to be fixed length, increase the search speed for analyzer, but also not efficient in term of diskspace.</p>

<ul>
<li>search</li>
</ul>

<p>The search function is using binary search + index for first word. Search function will jump to to starting position of first alphabat by refering index, and than search midpoint, if the word in midpoint is smaller than query, search again in upper side, search below part it is larger.</p>

<p>the search reaction speed depends on the number of words in query, it is avarage around 1~3 sec in nz10 records now, 5~10 sec before without index, and several minutes without binary search.</p>

<p>However, it still cause log10 IO access in searching, using intervial read + binary search will be more efficient in IO usage.</p>

<ul>
<li>relativity</li>
</ul>

<p>the relativity and ranking method is using BM25, when analyze mulitple words, simply add the total BM score in different words.</p>

<p>The BM formula:</p>

<pre><code>K =  K1 * ((1 - b) + b) * total_word_freq / avg_size 
BM = math.log10(doc_number - total_word_freq + 0.5) / (total_word_freq + 0.5) * ((K1 +1) * freq )/ (K + freq)</code></pre>

<p>Free constant K1 is 1.2 , b is 0.75</p>

<h2 id='improvment'>Improvment</h2>

<ul>
<li>Compression of index file</li>
</ul>

<p>The wordId in index can be reduced by combining the words with same id, aslo index file may be compress in chunk, in compression method such as Simple9 The binary search will also able to search in fixed size chunk also reduce the disk IO usage</p>

<ul>
<li>Implementation of Ranking function</li>
</ul>

<p>The result of the query is not really useful, the ranking function like page rank or HITS may provide better results.</p>
</body></html>
